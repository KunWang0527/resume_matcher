{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd12c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Python path: c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\n",
      "Looking for parsers module in: c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\parsers\n",
      "Parsers directory exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base parser parsed 47 resumes\n",
      "Job Title: Software Engineer - Backend\n",
      "Required Skills: ['Python', 'Java', 'SQL', 'REST API Development', 'AWS', 'Docker', 'Git']\n",
      "\n",
      "Sample Parsed Resume (Base):\n",
      "{\n",
      "  \"contact\": {\n",
      "    \"name\": \"Wendy Bailey\",\n",
      "    \"email\": \"w.bailey@email.com\",\n",
      "    \"phone\": \"(123) 456-7890\",\n",
      "    \"linkedin\": null,\n",
      "    \"github\": null,\n",
      "    \"location\": \"Philadelphia, PA\",\n",
      "    \"website\": null\n",
      "  },\n",
      "  \"summary\": null,\n",
      "  \"skills\": {\n",
      "    \"all\": [\n",
      "      \"Microservices\",\n",
      "      \"SQL\",\n",
      "      \"AWS\",\n",
      "      \"Python\",\n",
      "      \"Management\",\n",
      "      \"PostgreSQL\",\n",
      "      \"Angular\",\n",
      "      \"Kubernetes\",\n",
      "      \"Team\"\n",
      "    ],\n",
      "    \"technical\": [\n",
      "      \"Microservices\",\n",
      "      \"SQL\",\n",
      "      \"AWS\",\n",
      "      \"Python\",\n",
      "      \"PostgreSQL\",\n",
      "      \"Angular\",\n",
      "      \"Kubernetes\"\n",
      "    ],\n",
      "    \"soft\": [\n",
      "      \"Management\",\n",
      "      \"Team\"\n",
      "    ],\n",
      "    \"languages\": [\n",
      "      \"Python\",\n",
      "      \"SQL\"\n",
      "    ],\n",
      "    \"tools\": [\n",
      "      \"Kubernetes\",\n",
      "      \"Angular\",\n",
      "      \"AWS\"\n",
      "    ],\n",
      "    \"databases\": [\n",
      "      \"PostgreSQL\"\n",
      "    ]\n",
      "  },\n",
      "  \"experience\": [\n",
      "    {\n",
      "      \"company\": null,\n",
      "      \"title\": \"ware Engineer\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"Engineer\\n2023\",\n",
      "      \"end_date\": \"current\",\n",
      "      \"description\": []\n",
      "    },\n",
      "    {\n",
      "      \"company\": null,\n",
      "      \"title\": \"ware Developer\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"20\",\n",
      "      \"end_date\": \"20\",\n",
      "      \"description\": []\n",
      "    },\n",
      "    {\n",
      "      \"company\": null,\n",
      "      \"title\": \"ware Developer\",\n",
      "      \"location\": null,\n",
      "      \"start_date\": \"20\",\n",
      "      \"end_date\": \"20\",\n",
      "      \"description\": []\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"Bachelor\",\n",
      "      \"field\": \"Science\",\n",
      "      \"school\": \"Carnegie Mellon University\",\n",
      "      \"location\": null,\n",
      "      \"graduation_year\": \"2017\",\n",
      "      \"gpa\": null,\n",
      "      \"honors\": []\n",
      "    }\n",
      "  ],\n",
      "  \"sections\": {\n",
      "    \"header\": \"Wendy Bailey\\nAmazon So\\u0000ware Engineer\\nw.bailey@email.com (123) 456-7890 Philadelphia, PA LinkedIn\",\n",
      "    \"experience\": \"Amazon - Amazon So\\u0000ware Engineer\\n2023 - current Philadelphia, PA\\nDeveloped and deployed 9 microservices using Kubernetes on AWS, increasing monthly email-\\nbased user satisfaction ratings by 7%.\\nAutomated regression and functional testing on Selenium, reducing manual testing by 6 hours.\\nReleased 2 new features for Amazon Prime, growing user subscriptions by 18% within three\\nmonths.\\nCreated custom Python scripts to automate infrastructure management tasks, saving 7 hours hours\\neach week.\\nDuolingo - So\\u0000ware Developer\\n2020 - 2023 Pittsburgh, PA\\nLed a team on IntelliJ IDEA to build 6 interactive language learning features that boosted monthly\\nuser engagement by 21%.\\nIntroduced Angular-based authentication features for the sign-up process, lowering monthly user\\ndropout rates by 24%.\\nCollaborated with 3 developers to review source codes on SVN, preventing code conflicts.\\nHeld pre-deployment checks with Travis CI, ensuring smooth updates while slashing post-release\\nhotfixes by 37%.\\nBNY Mellon - Junior So\\u0000ware Developer\\n2017 - 2020 Pittsburgh, PA\\nAssisted in designing a PostgreSQL database schema that cut down data retrieval speeds by 9\\nminutes.\\nOversaw complex SQL queries and routines for transaction processing, raising the system's\\nthroughput by 29%.\\nUsed Trello to organize project tasks, improving the team\\u2019s on-time project submissions by 48%.\\nContributed to developing a PostgreSQL-based fraud detection system, decreasing annual\\nfraudulent transactions by 9%.\",\n",
      "    \"education\": \"Carnegie Mellon University - Bachelor of Science, Computer Science\\n2013 - 2017 Pittsburgh, PA\",\n",
      "    \"skills\": \"Python; Subversion (SVN); IntelliJ IDEA; PostgreSQL; Amazon Web Services (AWS); Kubernetes; Travis CI;\\nAngular; Selenium; Trello\"\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"sections_detected\": [\n",
      "      \"header\",\n",
      "      \"experience\",\n",
      "      \"education\",\n",
      "      \"skills\"\n",
      "    ],\n",
      "    \"parsing_success\": true\n",
      "  },\n",
      "  \"resume_id\": \"amazon-software-engineer-resume-example\"\n",
      "}\n",
      "\n",
      "✅ Base parsed resumes saved to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\parsed_resumes.json\n",
      "\n",
      "🔑 OpenAI API key detected — also running GPT parser.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPT parsed resumes saved to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\parsed_resumes_gpt.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the Python path so we can import from parsers\n",
    "# Get current working directory and go up one level to reach project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    parent_dir = current_dir.parent\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "print(f\"Added to Python path: {parent_dir}\")\n",
    "print(f\"Looking for parsers module in: {parent_dir / 'parsers'}\")\n",
    "print(f\"Parsers directory exists: {(parent_dir / 'parsers').exists()}\")\n",
    "\n",
    "import json\n",
    "import openai\n",
    "import config\n",
    "from parsers.pdf_parser import pdf_to_text\n",
    "from parsers.base_parser import ResumeParser\n",
    "from parsers.gpt_parser import GPTResumeParser\n",
    "\n",
    "# Prefer config-driven key; fallback to openai.api_key if set elsewhere\n",
    "if config.OPENAI_API_KEY:\n",
    "    openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "resumes_dir = parent_dir / \"data\" / \"resumes\"\n",
    "job_desc_path = parent_dir / \"data\" / \"job_description.json\"\n",
    "\n",
    "# Collect PDF texts once\n",
    "pdf_texts = []\n",
    "for pdf_file in sorted(resumes_dir.glob(\"*.pdf\")):\n",
    "    resume_text = pdf_to_text(str(pdf_file))\n",
    "    if resume_text:\n",
    "        pdf_texts.append({\"resume_id\": pdf_file.stem, \"text\": resume_text})\n",
    "\n",
    "with open(job_desc_path, \"r\") as f:\n",
    "    job_data = json.load(f)\n",
    "\n",
    "# --- Always run Base Parser ---\n",
    "base_parser = ResumeParser()\n",
    "parsed_resumes_base = []\n",
    "for item in pdf_texts:\n",
    "    parsed = base_parser.parse(item[\"text\"])\n",
    "    parsed[\"resume_id\"] = item[\"resume_id\"]\n",
    "    parsed_resumes_base.append(parsed)\n",
    "\n",
    "print(f\"Base parser parsed {len(parsed_resumes_base)} resumes\")\n",
    "print(f\"Job Title: {job_data['title']}\")\n",
    "print(f\"Required Skills: {job_data['required_skills']}\")\n",
    "print(\"\\nSample Parsed Resume (Base):\")\n",
    "if parsed_resumes_base:\n",
    "    print(json.dumps(parsed_resumes_base[0], indent=2))\n",
    "\n",
    "# Save Base parsed resumes\n",
    "base_output = {\"resumes\": parsed_resumes_base, \"job_description\": job_data}\n",
    "base_path = parent_dir / \"data\" / \"parsed_resumes.json\"\n",
    "with open(base_path, \"w\") as f:\n",
    "    json.dump(base_output, f, indent=2)\n",
    "print(f\"\\n✅ Base parsed resumes saved to {base_path}\")\n",
    "\n",
    "# --- Optionally run GPT Parser ---\n",
    "if (openai.api_key or \"\").strip():\n",
    "    print(\"\\n🔑 OpenAI API key detected — also running GPT parser.\")\n",
    "    gpt_parser = GPTResumeParser(api_key=openai.api_key)\n",
    "    parsed_resumes_gpt = []\n",
    "    for item in pdf_texts:\n",
    "        try:\n",
    "            parsed_gpt = gpt_parser.parse_resume(item[\"text\"]) or {}\n",
    "        except Exception as e:\n",
    "            parsed_gpt = {\"error\": str(e)}\n",
    "        parsed_gpt[\"resume_id\"] = item[\"resume_id\"]\n",
    "        parsed_resumes_gpt.append(parsed_gpt)\n",
    "\n",
    "    gpt_output = {\"resumes\": parsed_resumes_gpt, \"job_description\": job_data}\n",
    "    gpt_path = parent_dir / \"data\" / \"parsed_resumes_gpt.json\"\n",
    "    with open(gpt_path, \"w\") as f:\n",
    "        json.dump(gpt_output, f, indent=2)\n",
    "    print(f\"✅ GPT parsed resumes saved to {gpt_path}\")\n",
    "else:\n",
    "    print(\"\\nℹ️ No OpenAI API key found — GPT parser skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49c8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 104.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final scored resumes (Base) saved to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\final_scored_resumes.json\n",
      "\n",
      "Top candidate (Base):\n",
      "{\n",
      "  \"resume_id\": \"resume_v4.21\",\n",
      "  \"name\": \"BOYANG LIU\",\n",
      "  \"rule_score\": 100,\n",
      "  \"semantic_score\": 59.33,\n",
      "  \"final_score\": 83.73,\n",
      "  \"suitability\": \"Suitable\",\n",
      "  \"breakdown\": {\n",
      "    \"skills_score\": 100,\n",
      "    \"education_score\": 0,\n",
      "    \"experience_score\": 0,\n",
      "    \"projects_score\": 0,\n",
      "    \"company_score\": 0\n",
      "  },\n",
      "  \"matched_skills\": [\n",
      "    \"docker\",\n",
      "    \"git\",\n",
      "    \"java\",\n",
      "    \"rest api\",\n",
      "    \"sql\",\n",
      "    \"aws\",\n",
      "    \"rest api development\",\n",
      "    \"python\",\n",
      "    \"amazon web services\",\n",
      "    \"rest\"\n",
      "  ],\n",
      "  \"missing_skills\": [\n",
      "    \"api\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 78.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final scored resumes (GPT) saved to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\final_scored_resumes_gpt.json\n",
      "\n",
      "Top candidate (GPT):\n",
      "{\n",
      "  \"resume_id\": \"python-developer-resume-example\",\n",
      "  \"name\": \"Giulia Gonzalez\",\n",
      "  \"rule_score\": 100,\n",
      "  \"semantic_score\": 44.5,\n",
      "  \"final_score\": 77.8,\n",
      "  \"suitability\": \"Suitable\",\n",
      "  \"breakdown\": {\n",
      "    \"skills_score\": 100,\n",
      "    \"education_score\": 0,\n",
      "    \"experience_score\": 0,\n",
      "    \"projects_score\": 0,\n",
      "    \"company_score\": 0\n",
      "  },\n",
      "  \"matched_skills\": [\n",
      "    \"git\",\n",
      "    \"java\",\n",
      "    \"api\",\n",
      "    \"rest api\",\n",
      "    \"sql\",\n",
      "    \"aws\",\n",
      "    \"rest api development\",\n",
      "    \"python\",\n",
      "    \"amazon web services\",\n",
      "    \"rest\"\n",
      "  ],\n",
      "  \"missing_skills\": [\n",
      "    \"docker\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Resume Scoring and Matching\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the Python path (in case this cell runs independently)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    parent_dir = current_dir.parent\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "import json\n",
    "from models.semantic_matcher import SemanticMatcher\n",
    "from scoring.scorer import ResumeScorer\n",
    "from scoring.rule_based import RuleBasedScorer\n",
    "\n",
    "# === Load job description ===\n",
    "with open(parent_dir / \"data\" / \"job_description.json\", \"r\") as f:\n",
    "    job_data = json.load(f)\n",
    "\n",
    "# === Initialize Scorers ===\n",
    "rule_scorer = RuleBasedScorer()\n",
    "matcher = SemanticMatcher()\n",
    "scorer = ResumeScorer(suitable_threshold=0.7, maybe_threshold=0.5)\n",
    "\n",
    "\n",
    "def score_resumes(resumes_list, job):\n",
    "    results = []\n",
    "    for resume in resumes_list:\n",
    "        rule_score, breakdown, matched, missing = rule_scorer.score(resume, job)\n",
    "        semantic_score = matcher.compute_composite_score(job, resume)\n",
    "        final_score = round(0.6 * rule_score + 0.4 * semantic_score, 2)\n",
    "        suitability = scorer.classify(final_score / 100)  # expects normalized 0–1\n",
    "        results.append({\n",
    "            \"resume_id\": resume.get(\"resume_id\"),\n",
    "            \"name\": resume.get(\"name\") or resume.get(\"contact\", {}).get(\"name\"),\n",
    "            \"rule_score\": rule_score,\n",
    "            \"semantic_score\": round(semantic_score, 2),\n",
    "            \"final_score\": final_score,\n",
    "            \"suitability\": suitability,\n",
    "            \"breakdown\": breakdown,\n",
    "            \"matched_skills\": matched,\n",
    "            \"missing_skills\": missing\n",
    "        })\n",
    "    results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# === Score Base parsed resumes ===\n",
    "base_path = parent_dir / \"data\" / \"parsed_resumes.json\"\n",
    "with open(base_path, \"r\") as f:\n",
    "    base_data = json.load(f)\n",
    "base_resumes = base_data.get(\"resumes\", [])\n",
    "\n",
    "scored_resumes = score_resumes(base_resumes, job_data)\n",
    "base_out_path = parent_dir / \"data\" / \"final_scored_resumes.json\"\n",
    "with open(base_out_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"job_title\": job_data.get(\"title\"),\n",
    "        \"total_candidates\": len(scored_resumes),\n",
    "        \"scored_resumes\": scored_resumes\n",
    "    }, f, indent=2)\n",
    "print(f\"✅ Final scored resumes (Base) saved to {base_out_path}\")\n",
    "if scored_resumes:\n",
    "    print(\"\\nTop candidate (Base):\")\n",
    "    print(json.dumps(scored_resumes[0], indent=2))\n",
    "\n",
    "# === If GPT parsed exists, score GPT as well ===\n",
    "gpt_path = parent_dir / \"data\" / \"parsed_resumes_gpt.json\"\n",
    "if gpt_path.exists():\n",
    "    with open(gpt_path, \"r\") as f:\n",
    "        gpt_data = json.load(f)\n",
    "    gpt_resumes = gpt_data.get(\"resumes\", [])\n",
    "    scored_resumes_gpt = score_resumes(gpt_resumes, job_data)\n",
    "    gpt_out_path = parent_dir / \"data\" / \"final_scored_resumes_gpt.json\"\n",
    "    with open(gpt_out_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"job_title\": job_data.get(\"title\"),\n",
    "            \"total_candidates\": len(scored_resumes_gpt),\n",
    "            \"scored_resumes\": scored_resumes_gpt\n",
    "        }, f, indent=2)\n",
    "    print(f\"✅ Final scored resumes (GPT) saved to {gpt_out_path}\")\n",
    "    if scored_resumes_gpt:\n",
    "        print(\"\\nTop candidate (GPT):\")\n",
    "        print(json.dumps(scored_resumes_gpt[0], indent=2))\n",
    "else:\n",
    "    print(\"ℹ️ No GPT parsed resumes found — skipping GPT scoring.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa03075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates (Base):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>name</th>\n",
       "      <th>final_score</th>\n",
       "      <th>rule_score</th>\n",
       "      <th>semantic_score</th>\n",
       "      <th>suitability</th>\n",
       "      <th>matched_count</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>matched_skills</th>\n",
       "      <th>missing_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resume_v4.21</td>\n",
       "      <td>BOYANG LIU</td>\n",
       "      <td>83.73</td>\n",
       "      <td>100</td>\n",
       "      <td>59.33</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>docker, git, java, rest api, sql, aws, rest ap...</td>\n",
       "      <td>api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astrid_resume</td>\n",
       "      <td>Astrid Gao</td>\n",
       "      <td>82.00</td>\n",
       "      <td>100</td>\n",
       "      <td>55.00</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>git, java, rest api, sql, aws, rest api develo...</td>\n",
       "      <td>docker, api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lin Yang CV</td>\n",
       "      <td>Lin Yang</td>\n",
       "      <td>80.80</td>\n",
       "      <td>99</td>\n",
       "      <td>53.50</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>git, java, rest api, sql, aws, rest api develo...</td>\n",
       "      <td>docker, api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>python-developer-resume-example</td>\n",
       "      <td>GIULIA WORK EXPERIENCE</td>\n",
       "      <td>80.23</td>\n",
       "      <td>100</td>\n",
       "      <td>50.58</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>git, java, rest api, sql, aws, rest api develo...</td>\n",
       "      <td>docker, api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java-software-engineer-resume-example</td>\n",
       "      <td>YVONNE GREEN WORK EXPERIENCE</td>\n",
       "      <td>79.45</td>\n",
       "      <td>100</td>\n",
       "      <td>48.62</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>docker, git, java, rest api, sql, aws, rest ap...</td>\n",
       "      <td>api, python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               resume_id                          name  \\\n",
       "0                           resume_v4.21                    BOYANG LIU   \n",
       "1                          Astrid_resume                    Astrid Gao   \n",
       "2                            Lin Yang CV                      Lin Yang   \n",
       "3        python-developer-resume-example        GIULIA WORK EXPERIENCE   \n",
       "4  java-software-engineer-resume-example  YVONNE GREEN WORK EXPERIENCE   \n",
       "\n",
       "   final_score  rule_score  semantic_score suitability  matched_count  \\\n",
       "0        83.73         100           59.33    Suitable             10   \n",
       "1        82.00         100           55.00    Suitable              9   \n",
       "2        80.80          99           53.50    Suitable              9   \n",
       "3        80.23         100           50.58    Suitable              9   \n",
       "4        79.45         100           48.62    Suitable              9   \n",
       "\n",
       "   missing_count                                     matched_skills  \\\n",
       "0              1  docker, git, java, rest api, sql, aws, rest ap...   \n",
       "1              2  git, java, rest api, sql, aws, rest api develo...   \n",
       "2              2  git, java, rest api, sql, aws, rest api develo...   \n",
       "3              2  git, java, rest api, sql, aws, rest api develo...   \n",
       "4              2  docker, git, java, rest api, sql, aws, rest ap...   \n",
       "\n",
       "  missing_skills  \n",
       "0            api  \n",
       "1    docker, api  \n",
       "2    docker, api  \n",
       "3    docker, api  \n",
       "4    api, python  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_traces(texttemplate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, textposition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutside\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(yaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Score\u001b[39m\u001b[38;5;124m'\u001b[39m, xaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCandidate\u001b[39m\u001b[38;5;124m'\u001b[39m, uniformtext_minsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, uniformtext_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhide\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\.venv\\lib\\site-packages\\plotly\\basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\.venv\\lib\\site-packages\\plotly\\io\\_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[0;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Visualization: Top 10 candidates (Base)\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Resolve project root\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "base_out_path = parent_dir / \"data\" / \"final_scored_resumes.json\"\n",
    "if not base_out_path.exists():\n",
    "    print(f\"Base results not found at {base_out_path}. Run the scoring cell first.\")\n",
    "else:\n",
    "    with open(base_out_path, \"r\") as f:\n",
    "        base_results = json.load(f)\n",
    "\n",
    "    candidates = base_results.get(\"scored_resumes\", [])\n",
    "    topk = candidates[:10]\n",
    "\n",
    "    # Build DataFrame for display\n",
    "    def join_list(lst, max_items=10):\n",
    "        if not isinstance(lst, list):\n",
    "            return \"\"\n",
    "        s = \", \".join(lst[:max_items])\n",
    "        if len(lst) > max_items:\n",
    "            s += \", …\"\n",
    "        return s\n",
    "\n",
    "    rows = []\n",
    "    for c in topk:\n",
    "        rows.append({\n",
    "            \"resume_id\": c.get(\"resume_id\"),\n",
    "            \"name\": c.get(\"name\"),\n",
    "            \"final_score\": c.get(\"final_score\"),\n",
    "            \"rule_score\": c.get(\"rule_score\"),\n",
    "            \"semantic_score\": c.get(\"semantic_score\"),\n",
    "            \"suitability\": c.get(\"suitability\"),\n",
    "            \"matched_count\": len(c.get(\"matched_skills\", [])),\n",
    "            \"missing_count\": len(c.get(\"missing_skills\", [])),\n",
    "            \"matched_skills\": join_list(c.get(\"matched_skills\", []), max_items=20),\n",
    "            \"missing_skills\": join_list(c.get(\"missing_skills\", []), max_items=20),\n",
    "        })\n",
    "\n",
    "    df_topk_base = pd.DataFrame(rows)\n",
    "    print(\"Top 10 candidates (Base):\")\n",
    "    display(df_topk_base)\n",
    "\n",
    "    # Bar chart of Final Scores\n",
    "    fig = px.bar(df_topk_base, x=\"name\", y=\"final_score\", color=\"suitability\",\n",
    "                 title=\"Top 10 Final Scores (Base)\", text=\"final_score\")\n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "    fig.update_layout(yaxis_title='Final Score', xaxis_title='Candidate', uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6730fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 candidates (GPT):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>name</th>\n",
       "      <th>final_score</th>\n",
       "      <th>rule_score</th>\n",
       "      <th>semantic_score</th>\n",
       "      <th>suitability</th>\n",
       "      <th>matched_count</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>matched_skills</th>\n",
       "      <th>missing_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python-developer-resume-example</td>\n",
       "      <td>Giulia Gonzalez</td>\n",
       "      <td>77.80</td>\n",
       "      <td>100</td>\n",
       "      <td>44.50</td>\n",
       "      <td>Suitable</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>git, java, api, rest api, sql, aws, rest api d...</td>\n",
       "      <td>docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astrid_resume</td>\n",
       "      <td>Astrid Gao</td>\n",
       "      <td>67.19</td>\n",
       "      <td>81</td>\n",
       "      <td>46.47</td>\n",
       "      <td>Maybe Suitable</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>git, java, sql, aws, rest api development, pyt...</td>\n",
       "      <td>rest api, docker, api, rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YaoyaoWang_Resume_NEU</td>\n",
       "      <td>Yaoyao (Renee) Wang</td>\n",
       "      <td>63.61</td>\n",
       "      <td>74</td>\n",
       "      <td>48.03</td>\n",
       "      <td>Maybe Suitable</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>git, java, sql, rest api development, python, ...</td>\n",
       "      <td>docker, api, aws, rest api, rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resume_Hao_Yang</td>\n",
       "      <td>Hao Yang</td>\n",
       "      <td>62.40</td>\n",
       "      <td>70</td>\n",
       "      <td>51.00</td>\n",
       "      <td>Maybe Suitable</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>docker, git, java, sql, aws, python, amazon we...</td>\n",
       "      <td>rest api, api, rest api development, rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KW_Resume</td>\n",
       "      <td>Kun Wang</td>\n",
       "      <td>62.23</td>\n",
       "      <td>70</td>\n",
       "      <td>50.59</td>\n",
       "      <td>Maybe Suitable</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>docker, git, java, sql, aws, python, amazon we...</td>\n",
       "      <td>rest api, api, rest api development, rest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         resume_id                 name  final_score  \\\n",
       "0  python-developer-resume-example      Giulia Gonzalez        77.80   \n",
       "1                    Astrid_resume           Astrid Gao        67.19   \n",
       "2            YaoyaoWang_Resume_NEU  Yaoyao (Renee) Wang        63.61   \n",
       "3                  Resume_Hao_Yang             Hao Yang        62.40   \n",
       "4                        KW_Resume             Kun Wang        62.23   \n",
       "\n",
       "   rule_score  semantic_score     suitability  matched_count  missing_count  \\\n",
       "0         100           44.50        Suitable             10              1   \n",
       "1          81           46.47  Maybe Suitable              7              4   \n",
       "2          74           48.03  Maybe Suitable              6              5   \n",
       "3          70           51.00  Maybe Suitable              7              4   \n",
       "4          70           50.59  Maybe Suitable              7              4   \n",
       "\n",
       "                                      matched_skills  \\\n",
       "0  git, java, api, rest api, sql, aws, rest api d...   \n",
       "1  git, java, sql, aws, rest api development, pyt...   \n",
       "2  git, java, sql, rest api development, python, ...   \n",
       "3  docker, git, java, sql, aws, python, amazon we...   \n",
       "4  docker, git, java, sql, aws, python, amazon we...   \n",
       "\n",
       "                              missing_skills  \n",
       "0                                     docker  \n",
       "1                rest api, docker, api, rest  \n",
       "2           docker, api, aws, rest api, rest  \n",
       "3  rest api, api, rest api development, rest  \n",
       "4  rest api, api, rest api development, rest  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_traces(texttemplate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, textposition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutside\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(yaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Score\u001b[39m\u001b[38;5;124m'\u001b[39m, xaxis_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCandidate\u001b[39m\u001b[38;5;124m'\u001b[39m, uniformtext_minsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, uniformtext_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhide\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\.venv\\lib\\site-packages\\plotly\\basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\.venv\\lib\\site-packages\\plotly\\io\\_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[0;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Visualization: Top 10 candidates (GPT)\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Resolve project root\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "gpt_out_path = parent_dir / \"data\" / \"final_scored_resumes_gpt.json\"\n",
    "if not gpt_out_path.exists():\n",
    "    print(f\"GPT results not found at {gpt_out_path}. Run the GPT scoring or provide an API key.\")\n",
    "else:\n",
    "    with open(gpt_out_path, \"r\") as f:\n",
    "        gpt_results = json.load(f)\n",
    "\n",
    "    candidates = gpt_results.get(\"scored_resumes\", [])\n",
    "    topk = candidates[:10]\n",
    "\n",
    "    def join_list(lst, max_items=10):\n",
    "        if not isinstance(lst, list):\n",
    "            return \"\"\n",
    "        s = \", \".join(lst[:max_items])\n",
    "        if len(lst) > max_items:\n",
    "            s += \", …\"\n",
    "        return s\n",
    "\n",
    "    rows = []\n",
    "    for c in topk:\n",
    "        rows.append({\n",
    "            \"resume_id\": c.get(\"resume_id\"),\n",
    "            \"name\": c.get(\"name\"),\n",
    "            \"final_score\": c.get(\"final_score\"),\n",
    "            \"rule_score\": c.get(\"rule_score\"),\n",
    "            \"semantic_score\": c.get(\"semantic_score\"),\n",
    "            \"suitability\": c.get(\"suitability\"),\n",
    "            \"matched_count\": len(c.get(\"matched_skills\", [])),\n",
    "            \"missing_count\": len(c.get(\"missing_skills\", [])),\n",
    "            \"matched_skills\": join_list(c.get(\"matched_skills\", []), max_items=20),\n",
    "            \"missing_skills\": join_list(c.get(\"missing_skills\", []), max_items=20),\n",
    "        })\n",
    "\n",
    "    df_topk_gpt = pd.DataFrame(rows)\n",
    "    print(\"Top 10 candidates (GPT):\")\n",
    "    display(df_topk_gpt)\n",
    "\n",
    "    fig = px.bar(df_topk_gpt, x=\"name\", y=\"final_score\", color=\"suitability\",\n",
    "                 title=\"Top 10 Final Scores (GPT)\", text=\"final_score\")\n",
    "    fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "    fig.update_layout(yaxis_title='Final Score', xaxis_title='Candidate', uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3626184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Top 10 Base candidates to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\top10_base_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Save Top 10 (Base) to CSV\n",
    "# Run this cell to export the Top 10 base candidates table to CSV\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "base_out_path = parent_dir / \"data\" / \"final_scored_resumes.json\"\n",
    "\n",
    "if not base_out_path.exists():\n",
    "    print(f\"Base results not found at {base_out_path}. Run the scoring and visualization cells first.\")\n",
    "else:\n",
    "    with open(base_out_path, \"r\") as f:\n",
    "        base_results = json.load(f)\n",
    "    candidates = base_results.get(\"scored_resumes\", [])[:10]\n",
    "\n",
    "    def join_list(lst, max_items=100):\n",
    "        if not isinstance(lst, list):\n",
    "            return \"\"\n",
    "        return \", \".join(lst[:max_items])\n",
    "\n",
    "    rows = []\n",
    "    for c in candidates:\n",
    "        rows.append({\n",
    "            \"resume_id\": c.get(\"resume_id\"),\n",
    "            \"name\": c.get(\"name\"),\n",
    "            \"final_score\": c.get(\"final_score\"),\n",
    "            \"rule_score\": c.get(\"rule_score\"),\n",
    "            \"semantic_score\": c.get(\"semantic_score\"),\n",
    "            \"suitability\": c.get(\"suitability\"),\n",
    "            \"matched_skills\": join_list(c.get(\"matched_skills\", [])),\n",
    "            \"missing_skills\": join_list(c.get(\"missing_skills\", [])),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = parent_dir / \"data\" / \"top10_base_candidates.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved Top 10 Base candidates to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0d6104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Top 10 GPT candidates to c:\\Users\\wangk\\OneDrive\\Desktop\\resume_matcher\\data\\top10_gpt_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Save Top 10 (GPT) to CSV\n",
    "# Run this cell to export the Top 10 GPT candidates table to CSV\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "gpt_out_path = parent_dir / \"data\" / \"final_scored_resumes_gpt.json\"\n",
    "\n",
    "if not gpt_out_path.exists():\n",
    "    print(f\"GPT results not found at {gpt_out_path}. Run the GPT scoring and visualization cells first.\")\n",
    "else:\n",
    "    with open(gpt_out_path, \"r\") as f:\n",
    "        gpt_results = json.load(f)\n",
    "    candidates = gpt_results.get(\"scored_resumes\", [])[:10]\n",
    "\n",
    "    def join_list(lst, max_items=100):\n",
    "        if not isinstance(lst, list):\n",
    "            return \"\"\n",
    "        return \", \".join(lst[:max_items])\n",
    "\n",
    "    rows = []\n",
    "    for c in candidates:\n",
    "        rows.append({\n",
    "            \"resume_id\": c.get(\"resume_id\"),\n",
    "            \"name\": c.get(\"name\"),\n",
    "            \"final_score\": c.get(\"final_score\"),\n",
    "            \"rule_score\": c.get(\"rule_score\"),\n",
    "            \"semantic_score\": c.get(\"semantic_score\"),\n",
    "            \"suitability\": c.get(\"suitability\"),\n",
    "            \"matched_skills\": join_list(c.get(\"matched_skills\", [])),\n",
    "            \"missing_skills\": join_list(c.get(\"missing_skills\", [])),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = parent_dir / \"data\" / \"top10_gpt_candidates.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved Top 10 GPT candidates to {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
